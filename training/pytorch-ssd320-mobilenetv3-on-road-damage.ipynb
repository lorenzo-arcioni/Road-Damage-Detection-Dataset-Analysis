{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c7a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T23:15:43.497843Z",
     "iopub.status.busy": "2025-10-12T23:15:43.497605Z",
     "iopub.status.idle": "2025-10-12T23:16:01.047244Z",
     "shell.execute_reply": "2025-10-12T23:16:01.046337Z"
    },
    "papermill": {
     "duration": 17.555366,
     "end_time": "2025-10-12T23:16:01.048451",
     "exception": false,
     "start_time": "2025-10-12T23:15:43.493085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cella 1: Import e configurazione\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights\n",
    "from torchvision.models.detection.ssd import SSDClassificationHead\n",
    "from torchvision.models.detection import _utils\n",
    "from torchvision.ops import box_iou\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from sklearn.model_selection import KFold\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# RiproducibilitÃ \n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Configurazione\n",
    "PROJECT = \"pothole-detector-NatureSR\"\n",
    "ENTITY = \"pothole-detector\"\n",
    "NUM_FOLDS = 4\n",
    "IOU_THRESHOLD = 0.7\n",
    "CONF_THRESHOLD = 0.25  # CORRETTO: aumentato da 0.01 a 0.25\n",
    "INPUT_SIZE = 320\n",
    "\n",
    "# Parametri training\n",
    "CONFIG = {\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"epochs\": 100,\n",
    "    \"patience\": 10,\n",
    "}\n",
    "\n",
    "CLASS_NAMES = [\"pothole\", \"crack\", \"manhole\"]\n",
    "\n",
    "# Login wandb\n",
    "wandb.login(key=\"<API_KEY>\")\n",
    "print(\"âœ“ Setup completato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48f3d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T23:16:01.056039Z",
     "iopub.status.busy": "2025-10-12T23:16:01.055461Z",
     "iopub.status.idle": "2025-10-12T23:16:01.591051Z",
     "shell.execute_reply": "2025-10-12T23:16:01.590035Z"
    },
    "papermill": {
     "duration": 0.540599,
     "end_time": "2025-10-12T23:16:01.592481",
     "exception": false,
     "start_time": "2025-10-12T23:16:01.051882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cella 2: Creazione modello\n",
    "def create_model(num_classes=4, size=320):\n",
    "    \"\"\"Crea SSD320 con MobileNetV3 backbone\"\"\"\n",
    "    model = ssdlite320_mobilenet_v3_large(weights=SSDLite320_MobileNet_V3_Large_Weights.COCO_V1)\n",
    "    \n",
    "    # Modifica testa classificazione\n",
    "    in_channels = _utils.retrieve_out_channels(model.backbone, (size, size))\n",
    "    num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "    \n",
    "    model.head.classification_head = SSDClassificationHead(\n",
    "        in_channels=in_channels,\n",
    "        num_anchors=num_anchors,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    model.transform.min_size = (size,)\n",
    "    model.transform.max_size = size\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test modello\n",
    "model = create_model()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"âœ“ Modello creato: {total_params:,} parametri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76385c7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T23:16:01.601424Z",
     "iopub.status.busy": "2025-10-12T23:16:01.601206Z",
     "iopub.status.idle": "2025-10-12T23:16:01.610747Z",
     "shell.execute_reply": "2025-10-12T23:16:01.610017Z"
    },
    "papermill": {
     "duration": 0.014761,
     "end_time": "2025-10-12T23:16:01.611740",
     "exception": false,
     "start_time": "2025-10-12T23:16:01.596979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cella 3: Dataset\n",
    "class PotholeDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_files, input_size=320):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_files = image_files\n",
    "        self.input_size = input_size\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        original_w, original_h = image.size\n",
    "\n",
    "        label_name = img_name.rsplit('.', 1)[0] + '.txt'\n",
    "        label_path = os.path.join(self.label_dir, label_name)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 9:\n",
    "                        class_id = int(parts[0])\n",
    "                        coords = list(map(float, parts[1:]))\n",
    "\n",
    "                        xs = coords[0::2]\n",
    "                        ys = coords[1::2]\n",
    "\n",
    "                        x_min = min(xs) * original_w\n",
    "                        y_min = min(ys) * original_h\n",
    "                        x_max = max(xs) * original_w\n",
    "                        y_max = max(ys) * original_h\n",
    "\n",
    "                        w = x_max - x_min\n",
    "                        h = y_max - y_min\n",
    "\n",
    "                        # CORRETTO: Filtro bounding box troppo piccole\n",
    "                        if w >= 5.0 and h >= 5.0:\n",
    "                            boxes.append([x_min, y_min, x_max, y_max])\n",
    "                            labels.append(class_id + 1)\n",
    "\n",
    "        # Resize immagine e box\n",
    "        image = image.resize((self.input_size, self.input_size))\n",
    "        scale_x = self.input_size / original_w\n",
    "        scale_y = self.input_size / original_h\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            boxes[:, [0, 2]] *= scale_x\n",
    "            boxes[:, [1, 3]] *= scale_y\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "print(\"âœ“ Dataset definito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f8cc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T23:16:01.618867Z",
     "iopub.status.busy": "2025-10-12T23:16:01.618679Z",
     "iopub.status.idle": "2025-10-12T23:16:01.805733Z",
     "shell.execute_reply": "2025-10-12T23:16:01.804878Z"
    },
    "papermill": {
     "duration": 0.191936,
     "end_time": "2025-10-12T23:16:01.806976",
     "exception": false,
     "start_time": "2025-10-12T23:16:01.615040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cella 4: Preparazione dataset e fold\n",
    "image_dir = \"/kaggle/input/road-damage-dataset-potholes-cracks-and-manholes/data/images\"\n",
    "label_dir = \"/kaggle/input/road-damage-dataset-potholes-cracks-and-manholes/data/labels\"\n",
    "\n",
    "# Lista file\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "random.shuffle(image_files)\n",
    "print(f\"Dataset: {len(image_files)} immagini\")\n",
    "\n",
    "# Crea fold\n",
    "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n",
    "fold_splits = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(image_files)):\n",
    "    train_files = [image_files[i] for i in train_idx]\n",
    "    val_files = [image_files[i] for i in val_idx]\n",
    "    fold_splits.append({'train': train_files, 'val': val_files})\n",
    "    print(f\"Fold {fold_idx}: {len(train_files)} train, {len(val_files)} val\")\n",
    "\n",
    "print(f\"âœ“ {NUM_FOLDS} fold creati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89b39d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T23:16:01.814919Z",
     "iopub.status.busy": "2025-10-12T23:16:01.814713Z",
     "iopub.status.idle": "2025-10-12T23:16:01.831922Z",
     "shell.execute_reply": "2025-10-12T23:16:01.831121Z"
    },
    "papermill": {
     "duration": 0.022485,
     "end_time": "2025-10-12T23:16:01.833021",
     "exception": false,
     "start_time": "2025-10-12T23:16:01.810536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cella 5: Funzioni di training e metriche\n",
    "def train_one_epoch(model, loader, optimizer, device, epoch):\n",
    "    \"\"\"Training singola epoca\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_classification_loss = 0\n",
    "    total_bbox_regression_loss = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1} Train\")\n",
    "    \n",
    "    for images, targets in pbar:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        # CORRETTO: Gradient clipping per stabilitÃ \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        total_classification_loss += loss_dict.get('classification', torch.tensor(0.0)).item()\n",
    "        total_bbox_regression_loss += loss_dict.get('bbox_regression', torch.tensor(0.0)).item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{losses.item():.3f}'})\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_classification_loss/n, total_bbox_regression_loss/n\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    \"\"\"Validazione con loss\"\"\"\n",
    "    model.train()  # Serve per calcolare le loss\n",
    "    total_loss = 0\n",
    "    total_classification_loss = 0\n",
    "    total_bbox_regression_loss = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Validation\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in pbar:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            total_loss += losses.item()\n",
    "            total_classification_loss += loss_dict.get('classification', torch.tensor(0.0)).item()\n",
    "            total_bbox_regression_loss += loss_dict.get('bbox_regression', torch.tensor(0.0)).item()\n",
    "            \n",
    "            pbar.set_postfix({'val_loss': f'{losses.item():.3f}'})\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_classification_loss/n, total_bbox_regression_loss/n\n",
    "\n",
    "def calculate_metrics(model, loader, device, conf_threshold=CONF_THRESHOLD):\n",
    "    \"\"\"Calcola mAP e metriche per classe\"\"\"\n",
    "    model.eval()\n",
    "    metric_map = MeanAveragePrecision(class_metrics=True)\n",
    "    \n",
    "    # Contatori per precision/recall per classe\n",
    "    class_tp = [0, 0, 0]\n",
    "    class_fp = [0, 0, 0]\n",
    "    class_fn = [0, 0, 0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(loader, desc=\"Calculating metrics\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            preds = model(images)\n",
    "            \n",
    "            # Filtra per confidenza\n",
    "            filtered_preds = []\n",
    "            for pred in preds:\n",
    "                keep = pred['scores'] >= conf_threshold\n",
    "                filtered_preds.append({\n",
    "                    'boxes': pred['boxes'][keep].cpu(),\n",
    "                    'labels': pred['labels'][keep].cpu(),\n",
    "                    'scores': pred['scores'][keep].cpu()\n",
    "                })\n",
    "            \n",
    "            targets_cpu = [{k: v.cpu() for k, v in t.items()} for t in targets]\n",
    "            metric_map.update(filtered_preds, targets_cpu)\n",
    "            \n",
    "            # Calcola TP/FP/FN per classe\n",
    "            for pred, target in zip(filtered_preds, targets_cpu):\n",
    "                pred_boxes = pred['boxes']\n",
    "                pred_labels = pred['labels']\n",
    "                gt_boxes = target['boxes']\n",
    "                gt_labels = target['labels']\n",
    "                \n",
    "                if len(pred_boxes) == 0 and len(gt_boxes) == 0:\n",
    "                    continue\n",
    "                elif len(pred_boxes) == 0:\n",
    "                    for lbl in gt_labels:\n",
    "                        class_fn[lbl.item()-1] += 1\n",
    "                    continue\n",
    "                elif len(gt_boxes) == 0:\n",
    "                    for lbl in pred_labels:\n",
    "                        class_fp[lbl.item()-1] += 1\n",
    "                    continue\n",
    "                \n",
    "                ious = box_iou(pred_boxes, gt_boxes)\n",
    "                matched_gt = set()\n",
    "                \n",
    "                for pred_idx, pred_label in enumerate(pred_labels):\n",
    "                    best_iou = 0\n",
    "                    best_gt_idx = -1\n",
    "                    \n",
    "                    for gt_idx, gt_label in enumerate(gt_labels):\n",
    "                        if gt_idx in matched_gt:\n",
    "                            continue\n",
    "                        if pred_label.item() != gt_label.item():\n",
    "                            continue\n",
    "                        \n",
    "                        iou = ious[pred_idx, gt_idx].item()\n",
    "                        if iou > best_iou:\n",
    "                            best_iou = iou\n",
    "                            best_gt_idx = gt_idx\n",
    "                    \n",
    "                    cls_idx = pred_label.item() - 1\n",
    "                    if best_iou >= IOU_THRESHOLD and best_gt_idx != -1:\n",
    "                        class_tp[cls_idx] += 1\n",
    "                        matched_gt.add(best_gt_idx)\n",
    "                    else:\n",
    "                        class_fp[cls_idx] += 1\n",
    "                \n",
    "                for gt_idx, gt_label in enumerate(gt_labels):\n",
    "                    if gt_idx not in matched_gt:\n",
    "                        class_fn[gt_label.item()-1] += 1\n",
    "    \n",
    "    # Calcola metriche\n",
    "    map_results = metric_map.compute()\n",
    "    \n",
    "    # Metriche per classe\n",
    "    per_class = {}\n",
    "    for i, name in enumerate(CLASS_NAMES):\n",
    "        tp, fp, fn = class_tp[i], class_fp[i], class_fn[i]\n",
    "        p = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "        r = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "        f1 = 2*p*r/(p+r) if (p+r)>0 else 0.0\n",
    "        per_class[name] = {'precision': p, 'recall': r, 'f1': f1}\n",
    "    \n",
    "    # Metriche globali\n",
    "    total_tp = sum(class_tp)\n",
    "    total_fp = sum(class_fp)\n",
    "    total_fn = sum(class_fn)\n",
    "    \n",
    "    precision = total_tp/(total_tp+total_fp) if (total_tp+total_fp)>0 else 0.0\n",
    "    recall = total_tp/(total_tp+total_fn) if (total_tp+total_fn)>0 else 0.0\n",
    "    f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'mAP50': map_results['map_50'].item(),\n",
    "        'mAP50-95': map_results['map'].item(),\n",
    "        'per_class': per_class\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Funzioni training definite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423b805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T23:16:01.840438Z",
     "iopub.status.busy": "2025-10-12T23:16:01.840214Z",
     "iopub.status.idle": "2025-10-12T23:16:01.852402Z",
     "shell.execute_reply": "2025-10-12T23:16:01.851604Z"
    },
    "papermill": {
     "duration": 0.017204,
     "end_time": "2025-10-12T23:16:01.853521",
     "exception": false,
     "start_time": "2025-10-12T23:16:01.836317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cella 6: Training singolo fold\n",
    "def train_fold(fold_idx, fold_splits, config):\n",
    "    \"\"\"Training di un singolo fold\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_idx}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Dataset e loader\n",
    "    train_dataset = PotholeDataset(image_dir, label_dir, fold_splits[fold_idx]['train'], INPUT_SIZE)\n",
    "    val_dataset = PotholeDataset(image_dir, label_dir, fold_splits[fold_idx]['val'], INPUT_SIZE)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], \n",
    "                              shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], \n",
    "                           shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
    "    \n",
    "    # Modello e optimizer\n",
    "    model = create_model().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'],\n",
    "                         momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
    "    \n",
    "    # CORRETTO: Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])\n",
    "    \n",
    "    # Training loop\n",
    "    best_map = 0.0\n",
    "    patience_counter = 0\n",
    "    history = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training\n",
    "        train_loss, train_cls_loss, train_box_loss = train_one_epoch(\n",
    "            model, train_loader, optimizer, device, epoch\n",
    "        )\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_cls_loss, val_box_loss = validate(model, val_loader, device)\n",
    "        \n",
    "        # Learning rate\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Metriche ogni 5 epoche o ultima epoca\n",
    "        if (epoch + 1) % 5 == 0 or epoch == config['epochs'] - 1:\n",
    "            metrics = calculate_metrics(model, val_loader, device)\n",
    "            current_map = metrics['mAP50-95']\n",
    "        else:\n",
    "            metrics = None\n",
    "            current_map = 0.0\n",
    "        \n",
    "        # Salva history\n",
    "        epoch_data = {\n",
    "            'epoch': epoch + 1,\n",
    "            'time': time.time() - start_time,\n",
    "            'train/box_loss': train_box_loss,\n",
    "            'train/cls_loss': train_cls_loss,\n",
    "            'train/total_loss': train_loss,\n",
    "            'val/box_loss': val_box_loss,\n",
    "            'val/cls_loss': val_cls_loss,\n",
    "            'val/total_loss': val_loss,\n",
    "            'lr/pg0': lr,\n",
    "        }\n",
    "        \n",
    "        if metrics:\n",
    "            epoch_data.update({\n",
    "                'metrics/precision(B)': metrics['precision'],\n",
    "                'metrics/recall(B)': metrics['recall'],\n",
    "                'metrics/mAP50(B)': metrics['mAP50'],\n",
    "                'metrics/mAP50-95(B)': metrics['mAP50-95'],\n",
    "            })\n",
    "        \n",
    "        history.append(epoch_data)\n",
    "        \n",
    "        # Early stopping\n",
    "        if metrics and current_map > best_map:\n",
    "            best_map = current_map\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'fold{fold_idx}_best.pth')\n",
    "            print(f\"âœ“ Epoch {epoch+1}: New best mAP = {best_map:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= config['patience']:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Carica best model e calcola metriche finali\n",
    "    model.load_state_dict(torch.load(f'fold{fold_idx}_best.pth'))\n",
    "    final_metrics = calculate_metrics(model, val_loader, device)\n",
    "    \n",
    "    # Salva results.csv\n",
    "    df = pd.DataFrame(history)\n",
    "    df.to_csv(f'fold{fold_idx}_results.csv', index=False)\n",
    "    \n",
    "    # Prepara dati per .pt\n",
    "    class_logs = {}\n",
    "    for i, name in enumerate(CLASS_NAMES):\n",
    "        class_logs[f'fold{fold_idx}/precision_{name}'] = final_metrics['per_class'][name]['precision']\n",
    "        class_logs[f'fold{fold_idx}/recall_{name}'] = final_metrics['per_class'][name]['recall']\n",
    "        class_logs[f'fold{fold_idx}/f1_{name}'] = final_metrics['per_class'][name]['f1']\n",
    "    \n",
    "    class_logs.update({\n",
    "        f'fold{fold_idx}/precision_mean': final_metrics['precision'],\n",
    "        f'fold{fold_idx}/recall_mean': final_metrics['recall'],\n",
    "        f'fold{fold_idx}/f1_mean': final_metrics['f1'],\n",
    "        f'fold{fold_idx}/mAP50_mean': final_metrics['mAP50'],\n",
    "        f'fold{fold_idx}/mAP_mean': final_metrics['mAP50-95'],\n",
    "    })\n",
    "    \n",
    "    # Salva .pt\n",
    "    torch.save({\n",
    "        'fold': fold_idx,\n",
    "        'class_logs': class_logs,\n",
    "        'status': 'success',\n",
    "        'training_time': training_time\n",
    "    }, f'fold{fold_idx}_results.pt')\n",
    "    \n",
    "    print(f\"\\nâœ“ Fold {fold_idx} completato:\")\n",
    "    print(f\"  Precision: {final_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {final_metrics['recall']:.4f}\")\n",
    "    print(f\"  mAP@0.5: {final_metrics['mAP50']:.4f}\")\n",
    "    print(f\"  mAP@0.5:0.95: {final_metrics['mAP50-95']:.4f}\")\n",
    "    print(f\"  Training time: {training_time/60:.1f} min\")\n",
    "    \n",
    "    return final_metrics, training_time\n",
    "\n",
    "print(\"âœ“ Funzione train_fold definita\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea88cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T23:16:01.861096Z",
     "iopub.status.busy": "2025-10-12T23:16:01.860904Z",
     "iopub.status.idle": "2025-10-12T23:16:01.872390Z",
     "shell.execute_reply": "2025-10-12T23:16:01.871699Z"
    },
    "papermill": {
     "duration": 0.016693,
     "end_time": "2025-10-12T23:16:01.873517",
     "exception": false,
     "start_time": "2025-10-12T23:16:01.856824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cella 7: Cross-validation completa\n",
    "def cross_validation():\n",
    "    \"\"\"Esegue cross-validation su tutti i fold\"\"\"\n",
    "    print(f\"\\nðŸš€ Cross-Validation con {NUM_FOLDS} fold\")\n",
    "    \n",
    "    # âœ… Inizializza wandb SUBITO all'inizio\n",
    "    run = wandb.init(\n",
    "        project=PROJECT,\n",
    "        entity=ENTITY,\n",
    "        name=f\"Mobilenetv3-ssd320_cv_{NUM_FOLDS}fold\",\n",
    "        config=CONFIG\n",
    "    )\n",
    "    run_id = wandb.run.id\n",
    "    \n",
    "    all_results = []\n",
    "    total_start = time.time()\n",
    "    \n",
    "    # Disabilita wandb per singoli fold (cosÃ¬ non creano run separati)\n",
    "    os.environ['WANDB_MODE'] = 'disabled'\n",
    "    \n",
    "    # Training di tutti i fold\n",
    "    for fold_idx in range(NUM_FOLDS):\n",
    "        try:\n",
    "            metrics, train_time = train_fold(fold_idx, fold_splits, CONFIG)\n",
    "            all_results.append({\n",
    "                'fold': fold_idx,\n",
    "                'precision': metrics['precision'],\n",
    "                'recall': metrics['recall'],\n",
    "                'mAP50': metrics['mAP50'],\n",
    "                'mAP50-95': metrics['mAP50-95'],\n",
    "                'training_time_min': train_time/60\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Errore fold {fold_idx}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # âœ… Riabilita wandb\n",
    "    if 'WANDB_MODE' in os.environ:\n",
    "        del os.environ['WANDB_MODE']\n",
    "    \n",
    "    total_time = time.time() - total_start\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"âŒ Nessun fold completato\")\n",
    "        wandb.finish()\n",
    "        return False\n",
    "    \n",
    "    # Statistiche aggregate\n",
    "    df = pd.DataFrame(all_results)\n",
    "    summary = {\n",
    "        'successful_folds': len(all_results),\n",
    "        'total_time_hours': total_time/3600,\n",
    "        'precision_mean': df['precision'].mean(),\n",
    "        'precision_std': df['precision'].std(),\n",
    "        'recall_mean': df['recall'].mean(),\n",
    "        'recall_std': df['recall'].std(),\n",
    "        'mAP50_mean': df['mAP50'].mean(),\n",
    "        'mAP50_std': df['mAP50'].std(),\n",
    "        'mAP50-95_mean': df['mAP50-95'].mean(),\n",
    "        'mAP50-95_std': df['mAP50-95'].std(),\n",
    "    }\n",
    "    \n",
    "    # Salva summary\n",
    "    df.to_csv('cv_summary.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RISULTATI FINALI\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Fold completati: {len(all_results)}/{NUM_FOLDS}\")\n",
    "    print(f\"Tempo totale: {total_time/3600:.2f} ore\")\n",
    "    print(f\"\\nPrecision: {summary['precision_mean']:.4f} Â± {summary['precision_std']:.4f}\")\n",
    "    print(f\"Recall: {summary['recall_mean']:.4f} Â± {summary['recall_std']:.4f}\")\n",
    "    print(f\"mAP@0.5: {summary['mAP50_mean']:.4f} Â± {summary['mAP50_std']:.4f}\")\n",
    "    print(f\"mAP@0.5:0.95: {summary['mAP50-95_mean']:.4f} Â± {summary['mAP50-95_std']:.4f}\")\n",
    "    \n",
    "    # Log metriche aggregate nello STESSO run\n",
    "    wandb.log({\n",
    "        'cv/precision_mean': summary['precision_mean'],\n",
    "        'cv/recall_mean': summary['recall_mean'],\n",
    "        'cv/mAP50_mean': summary['mAP50_mean'],\n",
    "        'cv/mAP50-95_mean': summary['mAP50-95_mean'],\n",
    "        'experiment/successful_folds': len(all_results),\n",
    "        'experiment/total_time_hours': total_time/3600\n",
    "    })\n",
    "    \n",
    "    # Log metriche e crea artifact per ogni fold\n",
    "    for fold_idx in range(NUM_FOLDS):\n",
    "        pt_path = f'fold{fold_idx}_results.pt'\n",
    "        csv_path = f'fold{fold_idx}_results.csv'\n",
    "        pth_path = f'fold{fold_idx}_best.pth'\n",
    "        \n",
    "        if os.path.exists(pt_path):\n",
    "            data = torch.load(pt_path)\n",
    "            if data.get('status') == 'success':\n",
    "                wandb.log(data['class_logs'])\n",
    "                \n",
    "                # Crea artifact per questo fold\n",
    "                artifact = wandb.Artifact(f'fold{fold_idx}_results', type='model_results')\n",
    "                artifact.add_file(pt_path)\n",
    "                print(f\"âœ“ Fold {fold_idx}: Aggiunto {pt_path}\")\n",
    "                \n",
    "                if os.path.exists(csv_path):\n",
    "                    artifact.add_file(csv_path)\n",
    "                    print(f\"âœ“ Fold {fold_idx}: Aggiunto {csv_path}\")\n",
    "                \n",
    "                if os.path.exists(pth_path):\n",
    "                    artifact.add_file(pth_path)\n",
    "                    print(f\"âœ“ Fold {fold_idx}: Aggiunto {pth_path}\")\n",
    "                \n",
    "                # Carica artifact del fold\n",
    "                run.log_artifact(artifact).wait()\n",
    "                print(f\"ðŸ“¦ Fold {fold_idx}: Artifact caricato!\")\n",
    "    \n",
    "    # Carica summary come artifact separato\n",
    "    if os.path.exists('cv_summary.csv'):\n",
    "        summary_artifact = wandb.Artifact('cv_summary', type='summary')\n",
    "        summary_artifact.add_file('cv_summary.csv')\n",
    "        run.log_artifact(summary_artifact)\n",
    "        print(\"âœ“ Summary caricato!\")\n",
    "    \n",
    "    wandb.finish()  # âœ… Chiudi lo STESSO run\n",
    "    print(\"\\nâœ… Tutti gli artifact caricati su wandb!\")\n",
    "    return True\n",
    "\n",
    "print(\"âœ“ Funzione cross_validation definita\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd0bc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T23:16:01.880940Z",
     "iopub.status.busy": "2025-10-12T23:16:01.880730Z",
     "iopub.status.idle": "2025-10-12T23:52:41.633804Z",
     "shell.execute_reply": "2025-10-12T23:52:41.632991Z"
    },
    "papermill": {
     "duration": 2199.758064,
     "end_time": "2025-10-12T23:52:41.634887",
     "exception": false,
     "start_time": "2025-10-12T23:16:01.876823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cella 8: Esecuzione training\n",
    "if __name__ == \"__main__\":\n",
    "    success = cross_validation()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nðŸŽ‰ Training completato con successo!\")\n",
    "        print(\"\\nðŸ“ File generati per ogni fold:\")\n",
    "        for fold_idx in range(NUM_FOLDS):\n",
    "            print(f\"  - fold{fold_idx}_best.pth (modello)\")\n",
    "            print(f\"  - fold{fold_idx}_results.pt (metriche)\")\n",
    "            print(f\"  - fold{fold_idx}_results.csv (training log)\")\n",
    "        print(\"\\nðŸ“ File aggregati:\")\n",
    "        print(\"  - cv_summary.csv (statistiche cross-validation)\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Training fallito!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30070f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T23:52:44.024291Z",
     "iopub.status.busy": "2025-10-12T23:52:44.023482Z",
     "iopub.status.idle": "2025-10-12T23:52:46.144795Z",
     "shell.execute_reply": "2025-10-12T23:52:46.144099Z"
    },
    "papermill": {
     "duration": 3.367857,
     "end_time": "2025-10-12T23:52:46.147129",
     "exception": false,
     "start_time": "2025-10-12T23:52:42.779272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cella 9 (BONUS): Visualizzazione risultati\n",
    "def visualize_results():\n",
    "    \"\"\"Visualizza i risultati del training\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Carica summary\n",
    "    if not os.path.exists('cv_summary.csv'):\n",
    "        print(\"âŒ File cv_summary.csv non trovato\")\n",
    "        return\n",
    "    \n",
    "    summary = pd.read_csv('cv_summary.csv')\n",
    "    \n",
    "    # Plot metriche per fold\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    metrics = ['precision', 'recall', 'mAP50', 'mAP50-95']\n",
    "    titles = ['Precision', 'Recall', 'mAP@0.5', 'mAP@0.5:0.95']\n",
    "    \n",
    "    for ax, metric, title in zip(axes.flat, metrics, titles):\n",
    "        values = summary[metric].values\n",
    "        folds = summary['fold'].values\n",
    "        \n",
    "        ax.bar(folds, values, color='steelblue', alpha=0.7)\n",
    "        ax.axhline(values.mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {values.mean():.4f}')\n",
    "        ax.set_xlabel('Fold')\n",
    "        ax.set_ylabel(title)\n",
    "        ax.set_title(f'{title} per Fold')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cv_metrics.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ“ Grafico salvato come cv_metrics.png\")\n",
    "    \n",
    "    # Plot training curves per un fold\n",
    "    fold_to_plot = 0\n",
    "    csv_path = f'fold{fold_to_plot}_results.csv'\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss\n",
    "        ax = axes[0]\n",
    "        if 'train/total_loss' in df.columns:\n",
    "            ax.plot(df['epoch'], df['train/total_loss'], label='Train Loss', marker='o')\n",
    "        if 'val/total_loss' in df.columns:\n",
    "            ax.plot(df['epoch'], df['val/total_loss'], label='Val Loss', marker='s')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title(f'Fold {fold_to_plot} - Training & Validation Loss')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "        \n",
    "        # Metriche\n",
    "        ax = axes[1]\n",
    "        metric_cols = [col for col in df.columns if col.startswith('metrics/')]\n",
    "        for col in metric_cols:\n",
    "            values = df[col].dropna()\n",
    "            if len(values) > 0:\n",
    "                epochs = df.loc[values.index, 'epoch']\n",
    "                label = col.replace('metrics/', '').replace('(B)', '')\n",
    "                ax.plot(epochs, values, label=label, marker='o')\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Metric Value')\n",
    "        ax.set_title(f'Fold {fold_to_plot} - Metrics')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'fold{fold_to_plot}_curves.png', dpi=150)\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"âœ“ Grafico fold {fold_to_plot} salvato come fold{fold_to_plot}_curves.png\")\n",
    "\n",
    "# Visualizza i risultati (opzionale, decommentare per usare)\n",
    "visualize_results()\n",
    "\n",
    "print(\"âœ“ Funzione visualize_results definita (decommentare per usare)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376bfea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T23:52:48.553467Z",
     "iopub.status.busy": "2025-10-12T23:52:48.553102Z",
     "iopub.status.idle": "2025-10-12T23:52:48.561622Z",
     "shell.execute_reply": "2025-10-12T23:52:48.561034Z"
    },
    "papermill": {
     "duration": 1.16449,
     "end_time": "2025-10-12T23:52:48.562759",
     "exception": false,
     "start_time": "2025-10-12T23:52:47.398269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_predictions(model, val_dataset, device, num_images=20, score_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Esegue e visualizza le predizioni del modello su immagini di validazione in una griglia 2xN.\n",
    "    \n",
    "    Args:\n",
    "        model: Modello SSD PyTorch giÃ  caricato con i pesi.\n",
    "        val_dataset: Oggetto Dataset per la validazione (istanza di PotholeDataset).\n",
    "        device: torch.device (cuda o cpu).\n",
    "        num_images: Numero di immagini da predire e visualizzare.\n",
    "        score_threshold: Soglia minima per visualizzare una predizione.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    class_names = [\"background\"] + CLASS_NAMES\n",
    "    colors = [\"gray\", \"red\", \"orange\", \"blue\"]\n",
    "\n",
    "    indices = random.sample(range(len(val_dataset)), num_images)\n",
    "    n_cols = 2\n",
    "    n_rows = math.ceil(num_images / n_cols)\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(14, 6 * n_rows))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for plot_idx, dataset_idx in enumerate(indices):\n",
    "        image_tensor, _ = val_dataset[dataset_idx]\n",
    "        image = image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "        img_np = image_tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(image)[0]\n",
    "\n",
    "        ax = axs[plot_idx]\n",
    "        ax.imshow(img_np)\n",
    "\n",
    "        for box, label, score in zip(prediction[\"boxes\"], prediction[\"labels\"], prediction[\"scores\"]):\n",
    "            if score < score_threshold:\n",
    "                continue\n",
    "\n",
    "            box = box.cpu().numpy()\n",
    "            label = label.item()\n",
    "            score = score.item()\n",
    "\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "\n",
    "            rect = patches.Rectangle(\n",
    "                (x_min, y_min), width, height,\n",
    "                linewidth=2, edgecolor=colors[label], facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(\n",
    "                x_min, y_min - 5,\n",
    "                f\"{class_names[label]}: {score:.2f}\",\n",
    "                color=colors[label], fontsize=10,\n",
    "                backgroundcolor='black'\n",
    "            )\n",
    "\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Img: {val_dataset.image_files[dataset_idx]}\", fontsize=10)\n",
    "\n",
    "    # Disabilita eventuali assi non usati\n",
    "    for i in range(plot_idx + 1, len(axs)):\n",
    "        axs[i].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ef430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T23:52:51.062048Z",
     "iopub.status.busy": "2025-10-12T23:52:51.061760Z",
     "iopub.status.idle": "2025-10-12T23:53:03.442813Z",
     "shell.execute_reply": "2025-10-12T23:53:03.441908Z"
    },
    "papermill": {
     "duration": 13.973104,
     "end_time": "2025-10-12T23:53:03.783843",
     "exception": false,
     "start_time": "2025-10-12T23:52:49.810739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fold = 0  # o qualsiasi fold disponibile\n",
    "\n",
    "# Carica modello e pesi\n",
    "model = create_model(num_classes=4, size=320)\n",
    "model.load_state_dict(torch.load(f\"fold{fold}_best.pth\"))\n",
    "\n",
    "# Dataset di validazione\n",
    "val_dataset = PotholeDataset(\n",
    "    image_dir, label_dir,\n",
    "    fold_splits[fold]['val'],\n",
    "    input_size=320\n",
    ")\n",
    "\n",
    "# Visualizza le predizioni\n",
    "visualize_predictions(model, val_dataset, device, num_images=40, score_threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7723556,
     "sourceId": 12364584,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2248.059485,
   "end_time": "2025-10-12T23:53:08.190180",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-12T23:15:40.130695",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
